{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CopyGenerator(nn.Module):\n",
    "    \"\"\"An implementation of pointer-generator networks\n",
    "    :cite:`DBLP:journals/corr/SeeLM17`.\n",
    "\n",
    "    These networks consider copying words\n",
    "    directly from the source sequence.\n",
    "\n",
    "    The copy generator is an extended version of the standard\n",
    "    generator that computes three values.\n",
    "\n",
    "    * :math:`p_{softmax}` the standard softmax over `tgt_dict`\n",
    "    * :math:`p(z)` the probability of copying a word from\n",
    "      the source\n",
    "    * :math:`p_{copy}` the probility of copying a particular word.\n",
    "      taken from the attention distribution directly.\n",
    "\n",
    "    The model returns a distribution over the extend dictionary,\n",
    "    computed as\n",
    "\n",
    "    :math:`p(w) = p(z=1)  p_{copy}(w)  +  p(z=0)  p_{softmax}(w)`\n",
    "    Args:\n",
    "       input_size (int): size of input representation\n",
    "       output_size (int): size of output vocabulary\n",
    "       pad_idx (int)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, output_size, pad_idx):\n",
    "        super(CopyGenerator, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "        self.linear_copy = nn.Linear(input_size, 1)\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def forward(self, hidden, attn, src_map):\n",
    "        \"\"\"\n",
    "        Compute a distribution over the target dictionary\n",
    "        extended by the dynamic dictionary implied by copying\n",
    "        source words.\n",
    "\n",
    "        Args:\n",
    "           hidden (FloatTensor): hidden outputs ``(batch x tlen, input_size)``\n",
    "           attn (FloatTensor): attn for each ``(batch x tlen, input_size)``\n",
    "           src_map (FloatTensor):\n",
    "               A sparse indicator matrix mapping each source word to\n",
    "               its index in the \"extended\" vocab containing.\n",
    "               ``(src_len, batch, extra_words)``\n",
    "        \"\"\"\n",
    "\n",
    "        # CHECKS\n",
    "        batch_by_tlen, _ = hidden.size()\n",
    "        batch_by_tlen_, slen = attn.size()\n",
    "        slen_, batch, cvocab = src_map.size()\n",
    "        aeq(batch_by_tlen, batch_by_tlen_)\n",
    "        aeq(slen, slen_)\n",
    "\n",
    "        # Original probabilities.\n",
    "        logits = self.linear(hidden)\n",
    "        logits[:, self.pad_idx] = -float('inf')\n",
    "        prob = torch.softmax(logits, 1)\n",
    "\n",
    "        # Probability of copying p(z=1) batch.\n",
    "        p_copy = torch.sigmoid(self.linear_copy(hidden))\n",
    "        # Probability of not copying: p_{word}(w) * (1 - p(z))\n",
    "        out_prob = torch.mul(prob, 1 - p_copy)\n",
    "        mul_attn = torch.mul(attn, p_copy)\n",
    "        copy_prob = torch.bmm(\n",
    "            mul_attn.view(-1, batch, slen).transpose(0, 1),\n",
    "            src_map.transpose(0, 1)\n",
    "        ).transpose(0, 1)\n",
    "        copy_prob = copy_prob.contiguous().view(-1, cvocab)\n",
    "        return torch.cat([out_prob, copy_prob], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CopyGeneratorLoss(nn.Module):\n",
    "    \"\"\"Copy generator criterion.\"\"\"\n",
    "    def __init__(self, vocab_size, force_copy, unk_index=0,\n",
    "                 ignore_index=-100, eps=1e-20):\n",
    "        super(CopyGeneratorLoss, self).__init__()\n",
    "        self.force_copy = force_copy\n",
    "        self.eps = eps\n",
    "        self.vocab_size = vocab_size\n",
    "        self.ignore_index = ignore_index\n",
    "        self.unk_index = unk_index\n",
    "\n",
    "    def forward(self, scores, align, target):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            scores (FloatTensor): ``(batch_size*tgt_len)`` x dynamic vocab size\n",
    "                whose sum along dim 1 is less than or equal to 1, i.e. cols\n",
    "                softmaxed.\n",
    "            align (LongTensor): ``(batch_size x tgt_len)``\n",
    "            target (LongTensor): ``(batch_size x tgt_len)``\n",
    "        \"\"\"\n",
    "        # probabilities assigned by the model to the gold targets\n",
    "        vocab_probs = scores.gather(1, target.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # probability of tokens copied from source\n",
    "        # offset the indices by vocabulary size.\n",
    "        copy_ix = align.unsqueeze(1) + self.vocab_size\n",
    "        print(copy_ix)\n",
    "        copy_tok_probs = scores.gather(1, copy_ix).squeeze(1)\n",
    "        # Set scores for unk to 0 and add eps\n",
    "        # (those that should not be copied)\n",
    "        copy_tok_probs[align == self.unk_index] = 0\n",
    "        copy_tok_probs += self.eps  # to avoid -inf logs\n",
    "\n",
    "        # find the indices in which you do not use the copy mechanism\n",
    "        non_copy = align == self.unk_index\n",
    "        if not self.force_copy:\n",
    "            non_copy = non_copy | (target != self.unk_index)\n",
    "            \n",
    "        # If copy then use copy probs\n",
    "        # If non-copy then use vocab probs\n",
    "        probs = torch.where(\n",
    "            non_copy, copy_tok_probs + vocab_probs, copy_tok_probs\n",
    "        )\n",
    "\n",
    "        loss = -probs.log()  # just NLLLoss; can the module be incorporated?\n",
    "        # Drop padding.\n",
    "        loss[target == self.ignore_index] = 0\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 4\n",
    "batch_size = 3\n",
    "tgt_len = 5\n",
    "copy_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2., 3., 4., 5., 6.],\n",
       "        [0., 1., 2., 3., 4., 5., 6.],\n",
       "        [0., 1., 2., 3., 4., 5., 6.],\n",
       "        [0., 1., 2., 3., 4., 5., 6.],\n",
       "        [0., 1., 2., 3., 4., 5., 6.],\n",
       "        [0., 1., 2., 3., 4., 5., 6.],\n",
       "        [0., 1., 2., 3., 4., 5., 6.],\n",
       "        [0., 1., 2., 3., 4., 5., 6.],\n",
       "        [0., 1., 2., 3., 4., 5., 6.],\n",
       "        [0., 1., 2., 3., 4., 5., 6.],\n",
       "        [0., 1., 2., 3., 4., 5., 6.],\n",
       "        [0., 1., 2., 3., 4., 5., 6.],\n",
       "        [0., 1., 2., 3., 4., 5., 6.],\n",
       "        [0., 1., 2., 3., 4., 5., 6.],\n",
       "        [0., 1., 2., 3., 4., 5., 6.]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_scores = torch.zeros(batch_size * tgt_len, vocab_size + copy_size)\n",
    "for i in range(batch_size * tgt_len):\n",
    "    my_scores[i,:] = torch.arange(7)\n",
    "my_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 0, 1, 1, 0, 0, 1, 2, 1, 1, 0, 2, 0])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_align = torch.randint(low=0, high=vocab_size - 1, size=(batch_size * tgt_len,)).long().view(-1)\n",
    "my_align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 3, 1, 3, 2, 3, 1, 1, 0, 3, 3, 3, 0, 1, 2])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_target = torch.randint(low=0, high=tgt_len - 1, size=(batch_size * tgt_len,)).long().view(-1)\n",
    "my_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 3., 1., 3., 2., 3., 1., 1., 0., 3., 3., 3., 0., 1., 2.])\n",
      "tensor([[5],\n",
      "        [5],\n",
      "        [5],\n",
      "        [4],\n",
      "        [5],\n",
      "        [5],\n",
      "        [4],\n",
      "        [4],\n",
      "        [5],\n",
      "        [6],\n",
      "        [5],\n",
      "        [5],\n",
      "        [4],\n",
      "        [6],\n",
      "        [4]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-1.9459, -2.0794, -1.7918, -1.0986, -1.9459, -2.0794, -0.0000, -0.0000,\n",
       "        -1.6094, -2.1972, -2.0794, -2.0794, 46.0517, -1.9459, -0.6931])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = CopyGeneratorLoss(vocab_size, force_copy=False)\n",
    "loss(my_scores, my_align, my_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

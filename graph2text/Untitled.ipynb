{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CopyGeneratorLoss(nn.Module):\n",
    "    \"\"\"Copy generator criterion.\"\"\"\n",
    "    def __init__(self, vocab_size, force_copy, unk_index=-1,\n",
    "                 ignore_index=-100, eps=1e-20):\n",
    "        super(CopyGeneratorLoss, self).__init__()\n",
    "        self.force_copy = force_copy\n",
    "        self.eps = eps\n",
    "        self.vocab_size = vocab_size\n",
    "        self.ignore_index = ignore_index\n",
    "        self.unk_index = unk_index\n",
    "\n",
    "    def forward(self, scores, align, target):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            scores (FloatTensor): ``(batch_size*tgt_len)`` x dynamic vocab size\n",
    "                whose sum along dim 1 is less than or equal to 1, i.e. cols\n",
    "                softmaxed.\n",
    "            align (LongTensor): ``(batch_size x tgt_len)``\n",
    "            target (LongTensor): ``(batch_size x tgt_len)``\n",
    "        \"\"\"\n",
    "        # probabilities assigned by the model to the gold indices vocabulary tokens \n",
    "        vocab_probs = scores.gather(1, target.unsqueeze(1)).squeeze(1)\n",
    "        print(vocab_probs)\n",
    "        # probability of tokens copied from source\n",
    "        # offset the indices by vocabulary size.\n",
    "        copy_ix = align.unsqueeze(1) + self.vocab_size\n",
    "        print(copy_ix)\n",
    "        copy_tok_probs = scores.gather(1, copy_ix).squeeze(1)\n",
    "        print(copy_tok_probs)\n",
    "        # Set scores for unk to 0 and add eps\n",
    "        # (those that should not be copied)\n",
    "        copy_tok_probs[align == self.unk_index] = 0\n",
    "        copy_tok_probs += self.eps  # to avoid -inf logs\n",
    "\n",
    "        # find the indices in which you do not use the copy mechanism\n",
    "        non_copy = align == self.unk_index # tensor([-1,  1,  2, -1, -1, -1, -1])\n",
    "        print(non_copy)\n",
    "        print(self.unk_index)\n",
    "            \n",
    "        # If copy then use copy probs\n",
    "        # If non-copy then use vocab probs\n",
    "        probs = torch.where(\n",
    "            non_copy, copy_tok_probs + vocab_probs, copy_tok_probs\n",
    "        )\n",
    "        print(probs)\n",
    "\n",
    "        loss = -probs.log()  # just NLLLoss; can the module be incorporated?\n",
    "        # Drop padding.\n",
    "        loss[target == self.ignore_index] = 0\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 4 # special tokens\n",
    "batch_size = 1\n",
    "tgt_len = 7\n",
    "copy_size = 3  # input entity embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2., 3., 4., 5., 6.],\n",
       "        [0., 1., 2., 3., 4., 5., 6.],\n",
       "        [0., 1., 2., 3., 4., 5., 6.],\n",
       "        [0., 1., 2., 3., 4., 5., 6.],\n",
       "        [0., 1., 2., 3., 4., 5., 6.],\n",
       "        [0., 1., 2., 3., 4., 5., 6.],\n",
       "        [0., 1., 2., 3., 4., 5., 6.]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_scores = torch.zeros(batch_size * tgt_len, vocab_size + copy_size)\n",
    "for i in range(batch_size * tgt_len):\n",
    "    my_scores[i,:] = torch.arange(7)\n",
    "my_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_plan = torch.tensor([-2, 1, 2, -1, -1, -3, -4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 4, 4, 3, 3, 1, 0])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_target = torch.randint(low=0, high=vocab_size - 1, size=(batch_size * tgt_len,)).long().view(-1)\n",
    "my_target = torch.where(tgt_plan < 0, tgt_plan, 0) + 4\n",
    "my_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1,  1,  2, -1, -1, -1, -1])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_align = torch.randint(low=-1, high=copy_size - 1, size=(batch_size * tgt_len,)).long().view(-1)\n",
    "my_align = torch.where(tgt_plan >= 0, tgt_plan, -1)\n",
    "my_align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 4., 4., 3., 3., 1., 0.])\n",
      "tensor([[3],\n",
      "        [5],\n",
      "        [6],\n",
      "        [3],\n",
      "        [3],\n",
      "        [3],\n",
      "        [3]])\n",
      "tensor([3., 5., 6., 3., 3., 3., 3.])\n",
      "tensor([ True, False, False,  True,  True,  True,  True])\n",
      "-1\n",
      "tensor([2.0000e+00, 5.0000e+00, 6.0000e+00, 3.0000e+00, 3.0000e+00, 1.0000e+00,\n",
      "        1.0000e-20])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.6931, -1.6094, -1.7918, -1.0986, -1.0986, -0.0000, 46.0517])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = CopyGeneratorLoss(vocab_size, force_copy=False)\n",
    "loss(my_scores, my_align, my_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CopyGenerator(nn.Module):\n",
    "    \"\"\"An implementation of pointer-generator networks\n",
    "    :cite:`DBLP:journals/corr/SeeLM17`.\n",
    "\n",
    "    These networks consider copying words\n",
    "    directly from the source sequence.\n",
    "\n",
    "    The copy generator is an extended version of the standard\n",
    "    generator that computes three values.\n",
    "\n",
    "    * :math:`p_{softmax}` the standard softmax over `tgt_dict`\n",
    "    * :math:`p(z)` the probability of copying a word from\n",
    "      the source\n",
    "    * :math:`p_{copy}` the probility of copying a particular word.\n",
    "      taken from the attention distribution directly.\n",
    "\n",
    "    The model returns a distribution over the extend dictionary,\n",
    "    computed as\n",
    "\n",
    "    :math:`p(w) = p(z=1)  p_{copy}(w)  +  p(z=0)  p_{softmax}(w)`\n",
    "    Args:\n",
    "       input_size (int): size of input representation\n",
    "       output_size (int): size of output vocabulary\n",
    "       pad_idx (int)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, output_size, pad_idx):\n",
    "        super(CopyGenerator, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "        self.linear_copy = nn.Linear(input_size, 1)\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def forward(self, hidden, attn, src_map):\n",
    "        \"\"\"\n",
    "        Compute a distribution over the target dictionary\n",
    "        extended by the dynamic dictionary implied by copying\n",
    "        source words.\n",
    "\n",
    "        Args:\n",
    "           hidden (FloatTensor): hidden outputs ``(batch x tlen, input_size)``\n",
    "           attn (FloatTensor): attn for each ``(batch x tlen, input_size)``\n",
    "           src_map (FloatTensor):\n",
    "               A sparse indicator matrix mapping each source word to\n",
    "               its index in the \"extended\" vocab containing.\n",
    "               ``(src_len, batch, extra_words)``\n",
    "        \"\"\"\n",
    "\n",
    "        # CHECKS\n",
    "        batch_by_tlen, _ = hidden.size()\n",
    "        batch_by_tlen_, slen = attn.size()\n",
    "        slen_, batch, cvocab = src_map.size()\n",
    "        aeq(batch_by_tlen, batch_by_tlen_)\n",
    "        aeq(slen, slen_)\n",
    "\n",
    "        # Original probabilities.\n",
    "        logits = self.linear(hidden)\n",
    "        logits[:, self.pad_idx] = -float('inf')\n",
    "        prob = torch.softmax(logits, 1)\n",
    "\n",
    "        # Probability of copying p(z=1) batch.\n",
    "        p_copy = torch.sigmoid(self.linear_copy(hidden))\n",
    "        # Probability of not copying: p_{word}(w) * (1 - p(z))\n",
    "        out_prob = torch.mul(prob, 1 - p_copy)\n",
    "        mul_attn = torch.mul(attn, p_copy)\n",
    "        copy_prob = torch.bmm(\n",
    "            mul_attn.view(-1, batch, slen).transpose(0, 1),\n",
    "            src_map.transpose(0, 1)\n",
    "        ).transpose(0, 1)\n",
    "        copy_prob = copy_prob.contiguous().view(-1, cvocab)\n",
    "        return torch.cat([out_prob, copy_prob], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 4  # doc_start, edu_end, doc_end, pad\n",
    "pad_idx = -4\n",
    "input_size = 5\n",
    "batch_size = 2\n",
    "tlen = 3 # sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_generator = CopyGenerator(input_size, vocab_size, pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8035, 0.3979, 0.7843, 0.4679, 0.8035],\n",
       "        [0.7226, 0.4758, 0.2203, 0.0088, 0.0677],\n",
       "        [0.8937, 0.3917, 0.6902, 0.9131, 0.7289],\n",
       "        [0.5425, 0.4195, 0.9561, 0.8335, 0.5336],\n",
       "        [0.9044, 0.2503, 0.8863, 0.9474, 0.1896],\n",
       "        [0.3519, 0.8827, 0.7603, 0.7895, 0.0222],\n",
       "        [0.4087, 0.5756, 0.2003, 0.7533, 0.4260],\n",
       "        [0.9416, 0.5946, 0.1377, 0.0032, 0.8395],\n",
       "        [0.0773, 0.6500, 0.5805, 0.1012, 0.8393],\n",
       "        [0.2741, 0.1434, 0.5465, 0.2128, 0.1882]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_emb = torch.zeros(batch_size * tgt_len, input_size)\n",
    "for i in range(batch_size * tgt_len):\n",
    "    input_emb[i,:] = torch.rand(input_size)\n",
    "input_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7473, 0.8555, 0.9333, 0.7546, 0.1779],\n",
       "        [0.5429, 0.1780, 0.7941, 0.3028, 0.5348],\n",
       "        [0.4624, 0.3347, 0.9468, 0.3113, 0.0423],\n",
       "        [0.6377, 0.8386, 0.8025, 0.1749, 0.7684],\n",
       "        [0.3613, 0.0896, 0.5275, 0.0442, 0.7404],\n",
       "        [0.5135, 0.1966, 0.0225, 0.9383, 0.2640],\n",
       "        [0.5849, 0.7887, 0.4949, 0.0173, 0.1940],\n",
       "        [0.7462, 0.0195, 0.9299, 0.6881, 0.3056],\n",
       "        [0.8489, 0.4010, 0.1593, 0.3089, 0.8572],\n",
       "        [0.2396, 0.0105, 0.7221, 0.2826, 0.1396]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn = torch.zeros(batch_size * tgt_len, input_size)\n",
    "for i in range(batch_size * tgt_len):\n",
    "    attn[i,:] = torch.rand(input_size)\n",
    "attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'src_map'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-e96d174deead>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcopy_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/kg2text/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'src_map'"
     ]
    }
   ],
   "source": [
    "copy_generator(input_emb, attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 4, 4, 3, 4, 4, 3, 4, 3, 1, 0, 0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1,  2,  3, -1,  5,  1, -1,  7, -1, -1, -1, -1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
